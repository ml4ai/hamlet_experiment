:experiment iterations 2000
:experiment do_ground_truth_eval 1        // compares theta-star.txt to states.txt
:experiment do_test_set_eval 1            // test_states.txt - calculate marginal likelihood (marg the state seq)

// when Factorial: state as single thing, dimensions of state as independent chains
:MODEL MODEL_TYPE JOINT                   // {JOINT, FACTORIAL}

:MODULE TRANSITION_PRIOR Known_transition_matrix  // pi.txt, pi0.txt
Known_transition_matrix J 4               // specifies number of latent states
Known_transition_matrix transition_matrix_file <path_to_pi>       // give directory name -- appends 'pi' to dir name
Known_transition_matrix initial_distribution_file <path_to_pi0>   // give directory name -- appends 'pi0' 
// data_directory/
//   pi/ground_truth.txt  ; JxJ matrix
//   pi0/ground_truth.txt ; J vector
// J is determined by the dimensions of the Known_transition_matrix that is read in

:MODULE DYNAMICS HMM

// :MODULE SIMIALRITY None // turns off LT model, so no need to specify lambda, no need to specify params searching over Continuous_state_model
:MODULE SIMILARITY Isotropic_exponential  // not used when lambda=0
Isotropic_exponential_similarity lambda 0.0

// The following is currently required, but shouldn_t be
// Ideally, have explicit switch for whether to use LT -- if 0, then all of this would be ignored
// D is the dimension of the *similarity space*
// K is the dimension of the data -- 
//      When generating: determined by :generate observables_dimension
//      When inferring: determined by dimensionality of data that is read in
//                      OR, once we specify known means/precision, would be determined by dimensions of matrices
:MODULE STATE Continuous                  
Continuous_state_model D 1                 // Number of dimensions of latent continuous states -- work if set to zero?
Continuous_state_model prior_precision 1.0 //
Continuous_state_model L 1                 // number of HMC leapfrog steps per iteration
Continuous_state_model epsilon 0.0001      // HMC leapfrog step size

// {Means,Linear} Means=ID of state is loc in emission space; Linear=linear map from state space to emission space
:MODULE EMISSION Means
// Normal or Dirichlet (e.g., in Bach data)
:MODULE MEANS Normal                       // mean of Normal , or Categorical
:MODULE NOISE Normal                       // precision of Normal , or drawing from Categorical

// Mean_prior - mean_prior.h/cpp  - base class that particular prior inherit from
// Normal_mean_prior is derived class
// create: 
//   Known_means class that inherits from Mean_prior
//   modify Normal_noise_model -- add a hyperparameter: fixed_h
//      look at HDP_transition_prior to see the kind of thing we want to do : flag that governs whether gamma is known

// Normal (0 mean) prior over emission Normal mean
Normal_mean_prior precision 0.01  // could make 
// Gamma prior over the precision of the emission Normal precision
// set a_h / b_h to be mean you want, and set a_h and b_h to large numbers (e.g., a_h=50000, b_h=10000 -> mean 5 with high precision)
Normal_noise_model a_h 1.0  
Normal_noise_model b_h 1.0

// TODO: to specify known means and precisions of Normal emissions
// :MODULE MEANS Known  //
// Normal_mean_known
//   
// :MODULE NOISE Known  //
// Normal_precision_known 
//    One for all cases: single KxK matrix
//    Different diagonal: single JxK matrix
//    Full: (J*K)xK matrix

// Only relevant a Linear emission model
// :MODULE WEIGHTS_PRIOR Known_weights        // looks for weights file in data directory

// Only relevant to Linear emission model
// Known_weights include_bias 1              // 


// for implementing Viterbi -- only useful when you know the transition matrix (where sampling is used) b/c Viterbi maximizes
// dynamics_model
//   markov_transition_model : 
//      .update_z : samples, holding transition matrix fixed
//      just need new function in markov_transition_model that implements Virterbi: maximizes instead of sampling

//      a : is identical to \pi if doing no_LT
// 

/*
argmax p(z | pi, Y)  // what Viterbi does
p(z | Y)   // would be nice to get (when decoding), marginalizes out pi

Forward-Backward: marginal distribution for each time step.
p(z_t | pi, Y, emission_params)

marginalize out variance of Normal, get
  t-distribution for emission

*/